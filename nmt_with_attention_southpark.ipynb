{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('southparklines/All-seasons.csv')\n",
    "questions = []\n",
    "answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, data.shape[0] - 1):\n",
    "    if data.iloc[i + 1][2] == 'Stan':\n",
    "        questions.append(data.iloc[i][3])\n",
    "        answers.append(data.iloc[i + 1][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Going away? For how long?\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forever.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"  \",\"\",text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
    "\n",
    "    text = text.strip()\n",
    "    text = '<start> ' + text + ' <end>'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> going away for how long <end>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> forever <end>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_sentence(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_questions = []\n",
    "clean_answers = []\n",
    "\n",
    "for q in questions:\n",
    "    clean_questions.append(preprocess_sentence(q))\n",
    "for a in answers:\n",
    "    clean_answers.append(preprocess_sentence(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 30\n",
    "min_length = 1\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_length and len(question.split()) <= max_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "shorted_q = []\n",
    "shorted_a = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_length and len(answer.split()) <= max_length:\n",
    "        shorted_a.append(answer)\n",
    "        shorted_q.append(short_questions_temp[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer\n",
    "def load_dataset(inp_lang, targ_lang):\n",
    "  # creating cleaned input, output pairs\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6806"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shorted_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(inp_lang, targ_lang):\n",
    "  # creating cleaned input, output pairs\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(shorted_q, shorted_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6806, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6806, 30)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5444 5444 1362 1362\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print(\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "37 ----> dude\n",
      "12 ----> we\n",
      "13 ----> are\n",
      "10 ----> not\n",
      "51 ----> gonna\n",
      "4004 ----> betray\n",
      "3 ----> you\n",
      "56 ----> come\n",
      "24 ----> on\n",
      "49 ----> did\n",
      "3 ----> you\n",
      "64 ----> see\n",
      "2247 ----> prometheus\n",
      "4 ----> i\n",
      "14 ----> do\n",
      "10 ----> not\n",
      "62 ----> think\n",
      "6 ----> the\n",
      "4005 ----> writers\n",
      "170 ----> even\n",
      "342 ----> knew\n",
      "17 ----> what\n",
      "9 ----> that\n",
      "45 ----> was\n",
      "61 ----> about\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "56 ----> did\n",
      "50 ----> kyle\n",
      "37 ----> know\n",
      "59 ----> about\n",
      "20 ----> this\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 30]), TensorShape([64, 30]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 30, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (64, 1024)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "    # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (64, 4755)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                      sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './nmt_with_attention_southpark/training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "          \n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start save checkpoint in Epoch:  0\n",
      "Epoch 1 Batch 0 Loss 2.4387\n",
      "Start save checkpoint in Epoch:  0\n",
      "Epoch 1 Loss 1.8609\n",
      "Time taken for 1 epoch 47.91422629356384 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  1\n",
      "Epoch 2 Batch 0 Loss 1.7226\n",
      "Epoch 2 Loss 1.6451\n",
      "Time taken for 1 epoch 14.18011474609375 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  2\n",
      "Epoch 3 Batch 0 Loss 1.6222\n",
      "Epoch 3 Loss 1.5203\n",
      "Time taken for 1 epoch 14.194290161132812 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  3\n",
      "Epoch 4 Batch 0 Loss 1.3435\n",
      "Epoch 4 Loss 1.4479\n",
      "Time taken for 1 epoch 14.382851600646973 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  4\n",
      "Epoch 5 Batch 0 Loss 1.4458\n",
      "Epoch 5 Loss 1.3896\n",
      "Time taken for 1 epoch 14.340082883834839 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  5\n",
      "Epoch 6 Batch 0 Loss 1.4589\n",
      "Epoch 6 Loss 1.3384\n",
      "Time taken for 1 epoch 14.361990928649902 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  6\n",
      "Epoch 7 Batch 0 Loss 1.1948\n",
      "Epoch 7 Loss 1.2917\n",
      "Time taken for 1 epoch 14.219427108764648 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  7\n",
      "Epoch 8 Batch 0 Loss 1.1117\n",
      "Epoch 8 Loss 1.2447\n",
      "Time taken for 1 epoch 14.151309490203857 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  8\n",
      "Epoch 9 Batch 0 Loss 0.9807\n",
      "Epoch 9 Loss 1.1991\n",
      "Time taken for 1 epoch 14.676661014556885 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  9\n",
      "Epoch 10 Batch 0 Loss 0.9019\n",
      "Epoch 10 Loss 1.1507\n",
      "Time taken for 1 epoch 14.462061405181885 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  10\n",
      "Epoch 11 Batch 0 Loss 0.9471\n",
      "Start save checkpoint in Epoch:  10\n",
      "Epoch 11 Loss 1.1000\n",
      "Time taken for 1 epoch 14.778512716293335 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  11\n",
      "Epoch 12 Batch 0 Loss 1.0708\n",
      "Epoch 12 Loss 1.0456\n",
      "Time taken for 1 epoch 14.19408369064331 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  12\n",
      "Epoch 13 Batch 0 Loss 0.9999\n",
      "Epoch 13 Loss 0.9918\n",
      "Time taken for 1 epoch 14.191616296768188 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  13\n",
      "Epoch 14 Batch 0 Loss 1.0093\n",
      "Epoch 14 Loss 0.9340\n",
      "Time taken for 1 epoch 14.167013168334961 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  14\n",
      "Epoch 15 Batch 0 Loss 0.8685\n",
      "Epoch 15 Loss 0.8721\n",
      "Time taken for 1 epoch 14.132912874221802 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  15\n",
      "Epoch 16 Batch 0 Loss 0.8205\n",
      "Epoch 16 Loss 0.8055\n",
      "Time taken for 1 epoch 14.193233013153076 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  16\n",
      "Epoch 17 Batch 0 Loss 0.6846\n",
      "Epoch 17 Loss 0.7381\n",
      "Time taken for 1 epoch 14.475492715835571 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  17\n",
      "Epoch 18 Batch 0 Loss 0.5706\n",
      "Epoch 18 Loss 0.6704\n",
      "Time taken for 1 epoch 14.31417179107666 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  18\n",
      "Epoch 19 Batch 0 Loss 0.5736\n",
      "Epoch 19 Loss 0.6000\n",
      "Time taken for 1 epoch 14.17592477798462 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  19\n",
      "Epoch 20 Batch 0 Loss 0.5737\n",
      "Epoch 20 Loss 0.5290\n",
      "Time taken for 1 epoch 14.197641372680664 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  20\n",
      "Epoch 21 Batch 0 Loss 0.4975\n",
      "Start save checkpoint in Epoch:  20\n",
      "Epoch 21 Loss 0.4600\n",
      "Time taken for 1 epoch 14.783539772033691 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  21\n",
      "Epoch 22 Batch 0 Loss 0.3082\n",
      "Epoch 22 Loss 0.3915\n",
      "Time taken for 1 epoch 14.211656093597412 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  22\n",
      "Epoch 23 Batch 0 Loss 0.2839\n",
      "Epoch 23 Loss 0.3271\n",
      "Time taken for 1 epoch 14.159019231796265 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  23\n",
      "Epoch 24 Batch 0 Loss 0.2099\n",
      "Epoch 24 Loss 0.2696\n",
      "Time taken for 1 epoch 14.153186321258545 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  24\n",
      "Epoch 25 Batch 0 Loss 0.1709\n",
      "Epoch 25 Loss 0.2203\n",
      "Time taken for 1 epoch 14.284976243972778 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  25\n",
      "Epoch 26 Batch 0 Loss 0.1541\n",
      "Epoch 26 Loss 0.1763\n",
      "Time taken for 1 epoch 14.5271475315094 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  26\n",
      "Epoch 27 Batch 0 Loss 0.1234\n",
      "Epoch 27 Loss 0.1398\n",
      "Time taken for 1 epoch 14.176662921905518 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  27\n",
      "Epoch 28 Batch 0 Loss 0.1050\n",
      "Epoch 28 Loss 0.1113\n",
      "Time taken for 1 epoch 14.180317878723145 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  28\n",
      "Epoch 29 Batch 0 Loss 0.0859\n",
      "Epoch 29 Loss 0.0883\n",
      "Time taken for 1 epoch 14.178818464279175 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  29\n",
      "Epoch 30 Batch 0 Loss 0.0753\n",
      "Epoch 30 Loss 0.0724\n",
      "Time taken for 1 epoch 14.166231870651245 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  30\n",
      "Epoch 31 Batch 0 Loss 0.0524\n",
      "Start save checkpoint in Epoch:  30\n",
      "Epoch 31 Loss 0.0599\n",
      "Time taken for 1 epoch 14.745920658111572 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  31\n",
      "Epoch 32 Batch 0 Loss 0.0478\n",
      "Epoch 32 Loss 0.0508\n",
      "Time taken for 1 epoch 14.173043727874756 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  32\n",
      "Epoch 33 Batch 0 Loss 0.0284\n",
      "Epoch 33 Loss 0.0442\n",
      "Time taken for 1 epoch 14.151893377304077 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  33\n",
      "Epoch 34 Batch 0 Loss 0.0307\n",
      "Epoch 34 Loss 0.0404\n",
      "Time taken for 1 epoch 14.148346424102783 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  34\n",
      "Epoch 35 Batch 0 Loss 0.0419\n",
      "Epoch 35 Loss 0.0369\n",
      "Time taken for 1 epoch 14.16762089729309 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  35\n",
      "Epoch 36 Batch 0 Loss 0.0319\n",
      "Epoch 36 Loss 0.0342\n",
      "Time taken for 1 epoch 14.16829252243042 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  36\n",
      "Epoch 37 Batch 0 Loss 0.0271\n",
      "Epoch 37 Loss 0.0318\n",
      "Time taken for 1 epoch 14.423148155212402 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  37\n",
      "Epoch 38 Batch 0 Loss 0.0215\n",
      "Epoch 38 Loss 0.0293\n",
      "Time taken for 1 epoch 14.460716247558594 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  38\n",
      "Epoch 39 Batch 0 Loss 0.0311\n",
      "Epoch 39 Loss 0.0301\n",
      "Time taken for 1 epoch 14.659536838531494 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  39\n",
      "Epoch 40 Batch 0 Loss 0.0326\n",
      "Epoch 40 Loss 0.0302\n",
      "Time taken for 1 epoch 14.52147889137268 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  40\n",
      "Epoch 41 Batch 0 Loss 0.0252\n",
      "Start save checkpoint in Epoch:  40\n",
      "Epoch 41 Loss 0.0295\n",
      "Time taken for 1 epoch 15.287269830703735 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  41\n",
      "Epoch 42 Batch 0 Loss 0.0195\n",
      "Epoch 42 Loss 0.0316\n",
      "Time taken for 1 epoch 14.63511061668396 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  42\n",
      "Epoch 43 Batch 0 Loss 0.0222\n",
      "Epoch 43 Loss 0.0318\n",
      "Time taken for 1 epoch 14.152498483657837 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  43\n",
      "Epoch 44 Batch 0 Loss 0.0293\n",
      "Epoch 44 Loss 0.0345\n",
      "Time taken for 1 epoch 14.15744948387146 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  44\n",
      "Epoch 45 Batch 0 Loss 0.0177\n",
      "Epoch 45 Loss 0.0338\n",
      "Time taken for 1 epoch 14.183117389678955 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  45\n",
      "Epoch 46 Batch 0 Loss 0.0403\n",
      "Epoch 46 Loss 0.0333\n",
      "Time taken for 1 epoch 14.20654559135437 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  46\n",
      "Epoch 47 Batch 0 Loss 0.0199\n",
      "Epoch 47 Loss 0.0353\n",
      "Time taken for 1 epoch 14.150620222091675 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  47\n",
      "Epoch 48 Batch 0 Loss 0.0391\n",
      "Epoch 48 Loss 0.0394\n",
      "Time taken for 1 epoch 14.155242919921875 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  48\n",
      "Epoch 49 Batch 0 Loss 0.0271\n",
      "Epoch 49 Loss 0.0404\n",
      "Time taken for 1 epoch 14.435279607772827 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  49\n",
      "Epoch 50 Batch 0 Loss 0.0245\n",
      "Epoch 50 Loss 0.0379\n",
      "Time taken for 1 epoch 14.907490253448486 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  50\n",
      "Epoch 51 Batch 0 Loss 0.0409\n",
      "Start save checkpoint in Epoch:  50\n",
      "Epoch 51 Loss 0.0353\n",
      "Time taken for 1 epoch 14.882275581359863 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  51\n",
      "Epoch 52 Batch 0 Loss 0.0245\n",
      "Epoch 52 Loss 0.0319\n",
      "Time taken for 1 epoch 14.205931425094604 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  52\n",
      "Epoch 53 Batch 0 Loss 0.0309\n",
      "Epoch 53 Loss 0.0287\n",
      "Time taken for 1 epoch 14.145393371582031 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  53\n",
      "Epoch 54 Batch 0 Loss 0.0123\n",
      "Epoch 54 Loss 0.0244\n",
      "Time taken for 1 epoch 14.192644596099854 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  54\n",
      "Epoch 55 Batch 0 Loss 0.0157\n",
      "Epoch 55 Loss 0.0213\n",
      "Time taken for 1 epoch 14.180747270584106 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  55\n",
      "Epoch 56 Batch 0 Loss 0.0198\n",
      "Epoch 56 Loss 0.0201\n",
      "Time taken for 1 epoch 14.168313980102539 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  56\n",
      "Epoch 57 Batch 0 Loss 0.0126\n",
      "Epoch 57 Loss 0.0193\n",
      "Time taken for 1 epoch 14.151925563812256 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  57\n",
      "Epoch 58 Batch 0 Loss 0.0108\n",
      "Epoch 58 Loss 0.0188\n",
      "Time taken for 1 epoch 14.175851583480835 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  58\n",
      "Epoch 59 Batch 0 Loss 0.0063\n",
      "Epoch 59 Loss 0.0186\n",
      "Time taken for 1 epoch 14.182181358337402 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  59\n",
      "Epoch 60 Batch 0 Loss 0.0179\n",
      "Epoch 60 Loss 0.0179\n",
      "Time taken for 1 epoch 14.148847103118896 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  60\n",
      "Epoch 61 Batch 0 Loss 0.0236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start save checkpoint in Epoch:  60\n",
      "Epoch 61 Loss 0.0179\n",
      "Time taken for 1 epoch 14.732213497161865 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  61\n",
      "Epoch 62 Batch 0 Loss 0.0069\n",
      "Epoch 62 Loss 0.0180\n",
      "Time taken for 1 epoch 14.18880844116211 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  62\n",
      "Epoch 63 Batch 0 Loss 0.0149\n",
      "Epoch 63 Loss 0.0171\n",
      "Time taken for 1 epoch 14.190485000610352 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  63\n",
      "Epoch 64 Batch 0 Loss 0.0070\n",
      "Epoch 64 Loss 0.0177\n",
      "Time taken for 1 epoch 14.166420459747314 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  64\n",
      "Epoch 65 Batch 0 Loss 0.0130\n",
      "Epoch 65 Loss 0.0182\n",
      "Time taken for 1 epoch 14.179907083511353 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  65\n",
      "Epoch 66 Batch 0 Loss 0.0164\n",
      "Epoch 66 Loss 0.0194\n",
      "Time taken for 1 epoch 14.174920558929443 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  66\n",
      "Epoch 67 Batch 0 Loss 0.0150\n",
      "Epoch 67 Loss 0.0212\n",
      "Time taken for 1 epoch 14.165316343307495 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  67\n",
      "Epoch 68 Batch 0 Loss 0.0120\n",
      "Epoch 68 Loss 0.0253\n",
      "Time taken for 1 epoch 14.150546312332153 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  68\n",
      "Epoch 69 Batch 0 Loss 0.0190\n",
      "Epoch 69 Loss 0.0359\n",
      "Time taken for 1 epoch 14.149921894073486 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  69\n",
      "Epoch 70 Batch 0 Loss 0.0290\n",
      "Epoch 70 Loss 0.0507\n",
      "Time taken for 1 epoch 14.17075228691101 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  70\n",
      "Epoch 71 Batch 0 Loss 0.0445\n",
      "Start save checkpoint in Epoch:  70\n",
      "Epoch 71 Loss 0.0612\n",
      "Time taken for 1 epoch 14.99048900604248 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  71\n",
      "Epoch 72 Batch 0 Loss 0.0600\n",
      "Epoch 72 Loss 0.0527\n",
      "Time taken for 1 epoch 14.17613410949707 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  72\n",
      "Epoch 73 Batch 0 Loss 0.0201\n",
      "Epoch 73 Loss 0.0409\n",
      "Time taken for 1 epoch 14.15947151184082 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  73\n",
      "Epoch 74 Batch 0 Loss 0.0401\n",
      "Epoch 74 Loss 0.0321\n",
      "Time taken for 1 epoch 14.17514967918396 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  74\n",
      "Epoch 75 Batch 0 Loss 0.0489\n",
      "Epoch 75 Loss 0.0259\n",
      "Time taken for 1 epoch 14.145239353179932 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  75\n",
      "Epoch 76 Batch 0 Loss 0.0239\n",
      "Epoch 76 Loss 0.0217\n",
      "Time taken for 1 epoch 14.17723798751831 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  76\n",
      "Epoch 77 Batch 0 Loss 0.0154\n",
      "Epoch 77 Loss 0.0184\n",
      "Time taken for 1 epoch 14.17057204246521 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  77\n",
      "Epoch 78 Batch 0 Loss 0.0122\n",
      "Epoch 78 Loss 0.0175\n",
      "Time taken for 1 epoch 14.15770173072815 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  78\n",
      "Epoch 79 Batch 0 Loss 0.0113\n",
      "Epoch 79 Loss 0.0166\n",
      "Time taken for 1 epoch 14.162668228149414 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  79\n",
      "Epoch 80 Batch 0 Loss 0.0090\n",
      "Epoch 80 Loss 0.0159\n",
      "Time taken for 1 epoch 14.160638093948364 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  80\n",
      "Epoch 81 Batch 0 Loss 0.0097\n",
      "Start save checkpoint in Epoch:  80\n",
      "Epoch 81 Loss 0.0158\n",
      "Time taken for 1 epoch 14.696891069412231 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  81\n",
      "Epoch 82 Batch 0 Loss 0.0092\n",
      "Epoch 82 Loss 0.0151\n",
      "Time taken for 1 epoch 14.216941356658936 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  82\n",
      "Epoch 83 Batch 0 Loss 0.0150\n",
      "Epoch 83 Loss 0.0148\n",
      "Time taken for 1 epoch 14.164225101470947 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  83\n",
      "Epoch 84 Batch 0 Loss 0.0101\n",
      "Epoch 84 Loss 0.0147\n",
      "Time taken for 1 epoch 14.150763273239136 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  84\n",
      "Epoch 85 Batch 0 Loss 0.0090\n",
      "Epoch 85 Loss 0.0139\n",
      "Time taken for 1 epoch 14.178589344024658 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  85\n",
      "Epoch 86 Batch 0 Loss 0.0189\n",
      "Epoch 86 Loss 0.0141\n",
      "Time taken for 1 epoch 14.668338537216187 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  86\n",
      "Epoch 87 Batch 0 Loss 0.0051\n",
      "Epoch 87 Loss 0.0140\n",
      "Time taken for 1 epoch 14.568259716033936 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  87\n",
      "Epoch 88 Batch 0 Loss 0.0085\n",
      "Epoch 88 Loss 0.0152\n",
      "Time taken for 1 epoch 14.154191493988037 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  88\n",
      "Epoch 89 Batch 0 Loss 0.0071\n",
      "Epoch 89 Loss 0.0166\n",
      "Time taken for 1 epoch 14.194134950637817 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  89\n",
      "Epoch 90 Batch 0 Loss 0.0168\n",
      "Epoch 90 Loss 0.0167\n",
      "Time taken for 1 epoch 14.149745225906372 sec\n",
      "\n",
      "Start save checkpoint in Epoch:  90\n",
      "Epoch 91 Batch 0 Loss 0.0103\n",
      "Start save checkpoint in Epoch:  90\n",
      "Epoch 91 Loss 0.0175\n",
      "Time taken for 1 epoch 14.740186929702759 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 91\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  \n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(\"Start save checkpoint in Epoch: \", epoch)\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                   batch,\n",
    "                                                   batch_loss.numpy()))\n",
    "  # saving (checkpoint) the model every 2 epochs\n",
    "    if epoch%10 == 0:\n",
    "        print(\"Start save checkpoint in Epoch: \", epoch)\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=max_length_inp,\n",
    "                                                         padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                         dec_hidden,\n",
    "                                                         enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, _ = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "  #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  #plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30\n"
     ]
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "print(max_length_targ, max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> hi stan <end>\n",
      "Predicted translation: hi wendy <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"Hi Stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> going way <end>\n",
      "Predicted translation: oh no no <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"Going Way\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> what are you doing here <end>\n",
      "Predicted translation: please ih ih it will not take long <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"What are you doing here?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i was the vice president <end>\n",
      "Predicted translation: i know <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"i was the vice president\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> i do not get it <end>\n",
      "Predicted translation: me neither <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"i do not get it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> stan <end>\n",
      "Predicted translation: hello <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> it just does not seem right <end>\n",
      "Predicted translation: yeah our eyes are finally open dude it is like waking up for the first time <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"it just does not seem right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <start> guys can i talk to you <end>\n",
      "Predicted translation: sure dude <end> \n"
     ]
    }
   ],
   "source": [
    "translate(\"guys can i talk to you\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
